# Data Management for Machine Learning

<!--
## Data Management for Machine Learning
-->

Where and how to store data and ML model files is one of the first decisions
your team will face. But traditional back-up strategies do not fit the data
science lifecycle. Large files end up scattered throughout several buckets.
Overlapping dataset versions coexist, causing leakage and inefficient use of
space. The project evolution is harder to track. What was the name of the best
model? Is it safe to delete `huge_split.zip`? Can others reproduce my results?

![Direct access storage](/img/direct_access_storage.png) _The S3 bucket on the
right is shared (and bloated) by several people and projects. You need to know
the exact location of the correct files, and use cloud-specific tools (e.g. AWS
CLI) to access them directly._

To maintain control and visibility over all your data and models, DVC stores
large files and directories for you in a structured way. It tracks them by
logging their locations and unique descriptions in YAML files.

![DVC-cached storage](/img/dvc_managed_storage.png)

![]() _DVC writes `.dvc` files with YAML content next to large files. A data
cache indexes them with `md5` checksums. Mass storage holds al unique files
pushed with DVC for back up or sharing._

Committing the files generated by DVC to Git along your ML sources creates
reproducible project versions. The history becomes easy to review, rewind, and
repeat going forward (by anyone). There's no need to come up with special file
names for changed data or models.

![Versioning data with Git](/img/project_versioning.png) _You can use Git
history to store different dataset and model versions without renaming any files
in your workspace. The project cache grows as more relevant versions are
tracked._

## How it works

Let's consider a simplified, hypothetical ML project:

```
training.csv
validation.xml
model.bin
src/train.py
```

DVC appends unique large files to a hidden <abbr>cache</abbr> organized by
content hashes (similar to an index). As the data changes, its full history can
be preserved this way, while preventing accidental file deletions.

```cli
.dvc/cache
├── 0a/aa77e # training.csv
├── 3f/db533 # validation.xml before
├── 6a/2aa4b # validation.xml now
├── a7/28107 # first model.bin
    ...
```

Now that they're cached safely, DVC-tracked files in the <abbr>workspace</abbr>
can be replaced with [file links], so you continue seeing and using them as
usual. File hashes (usually MD5) are written in human-readable YAML [metafiles]
next to the original data.

```git
  training.csv -> .dvc/cache/0a/aa77e
+ training.csv.dvc
  validation.xml -> .dvc/cache/6a/2aa4b
+ validation.xml.dvc
  model.bin
  ...
```

```yaml
# validation.xml.dvc
  ...
  md5: 6a26845d4000daa4bfb196017e103355
  path: validation.xml
```

[metafiles]: /doc/user-guide/project-structure
[file links]: /doc/user-guide/data-management/large-dataset-optimization

<admon type="info" title="Data codification">

DVC replaces data assets in the project with code-like YAML [metafiles] (and
links). Codifying data lets you treat it as a first-class citizen in any code
repository.

</admon>

To keep track of relevant versions of the data, models, etc. cached by DVC, the
corresponding metafiles should be [versioned with Git] (or any SCM) along with
the rest of the code. This also means that a single file name can represent
different contents, keeping your project clean. You can use branches or tags to
organize data versions instead.

[versioned with git]:
  https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control

```cli
$ git checkout dev-branch
$ dvc checkout
$ ls
... training.csv      2 G # old data
... model.bin       2.7 M # old model
... src/train.py    214 K

$ git checkout latest-tag
$ dvc checkout
$ ls
... training.csv      3 G # latest data
... validation.xml    1 G
... model.bin       3.2 M # better model
... src/train.py    354 K
... src/evaluate.py 175 K # more code
```

Adopting DVC's approach requires a few key changes to your workflow:

1. Relevant data and models are registered in a code repository (typically Git).
1. Data operations (add, remove, move, etc.) happen [indirectly]: DVC checks the
   metadata to locate files in both sides.
1. Stored objects managed with DVC are not intended for handling manually.

[indirectly]: https://en.wikipedia.org/wiki/Indirection

<!-- ## Benefits and tradeoffs

At the same time, it comes with many benefits:

- Easily manage **data as code** and [optimize space usage][file links]
  automatically.
- DVC keeps track of large files and directories for you, mapping them between
  your <abbr>workspace</abbr> and storage.
- Easily share, distribute, and migrate data among one or more storage locations
  ([multiple providers supported]).
- Your <abbr>repository</abbr> stays small and easy **collaborate** on (using
  regular [Git workflows]).
- [Data versioning] guarantees ML **reproducibility**.
- Use a **consistent interface** to access and sync data anywhere (via [CLI],
  [API], [IDE], or [web]), regardless of the storage platform (S3, GDrive, NAS,
  etc.).
- Data **integrity** based on a Git-based storage; Data **security** through an
  authored project history that can be audited.
- Advanced features: [Data registries], [ML pipelines], [CI/CD for ML],
  [productize] your ML models, and more!

[multiple providers supported]:
  /doc/command-reference/remote/add#supported-storage-types
[git workflows]:
  https://git-scm.com/book/en/v2/Distributed-Git-Distributed-Workflows
[data versioning]: /doc/use-cases/versioning-data-and-models
[cli]: /doc/command-reference
[api]: /doc/api-reference
[ide]: /doc/vs-code-extension
[web]: /doc/studio
[data registries]: /doc/use-cases/data-registry
[ml pipelines]: /doc/user-guide/pipelines
[ci/cd for ml]: https://cml.dev/
[productize]: https://mlem.ai/
-->

## Storage locations

The cache is the first storage layer for you and your team to share and
collaborate, but more can be defined in DVC [config files] (using `dvc remote`
commands). These storage locations let you back up and share data, features, ML
models, etc. Supported platforms include SSH, Amazon S3, Google Cloud Storage,
Microsoft Azure, among [many more].

[config files]: /doc/user-guide/project-structure/internal-files
[many more]: /doc/command-reference/remote/add#supported-storage-types

---

In summary, DVC establishes a mature method to manage data assets for ML
projects, letting you focus on more important tasks like exploration,
preparation, cross validation, etc.
